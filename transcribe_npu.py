#!/usr/bin/env python3
"""
NPU-accelerated transcription script for Surface Copilot+ PCs
Uses ONNX Runtime with QNN Execution Provider for Qualcomm NPU acceleration.
"""

import argparse
import os
import sys
from pathlib import Path

def install_npu_dependencies():
    """Install NPU-specific dependencies if not already installed."""
    try:
        import onnxruntime
        # Check if QNN provider is available
        available_providers = onnxruntime.get_available_providers()
        if "QNNExecutionProvider" not in available_providers:
            print("Installing NPU support (onnxruntime-qnn)...")
            os.system("python -m pip install onnxruntime-qnn")
            print("NPU support installed. Please restart the script.")
            return False
    except ImportError:
        print("Installing ONNX Runtime with NPU support...")
        os.system("python -m pip install onnxruntime-qnn")
        print("NPU support installed. Please restart the script.")
        return False
    
    return True

def download_onnx_whisper_model(model_size="medium"):
    """Download ONNX Whisper model optimized for NPU."""
    from huggingface_hub import snapshot_download
    
    # Map model sizes to HuggingFace repo IDs
    model_repos = {
        "medium": "microsoft/whisper-medium",
        "small": "microsoft/whisper-small", 
        "large": "microsoft/whisper-large-v2"
    }
    
    if model_size not in model_repos:
        model_size = "medium"
    
    model_dir = Path.home() / ".cache" / "whisper-onnx" / model_size
    
    if not model_dir.exists():
        print(f"Downloading ONNX Whisper {model_size} model...")
        try:
            # This is a placeholder - actual ONNX models would need to be sourced
            # from a repository that has pre-quantized ONNX Whisper models
            print("NOTE: This is a prototype. Actual ONNX Whisper models need to be sourced.")
            print("For now, falling back to standard PyTorch Whisper with NPU simulation.")
            return None
        except Exception as e:
            print(f"Failed to download ONNX model: {e}")
            return None
    
    return model_dir

def transcribe_with_npu(audio_file, model_size="medium", output_dir=None):
    """Transcribe audio using NPU acceleration."""
    
    # For now, fall back to standard transcription with a note about NPU potential
    print("üöÄ NPU-Accelerated Transcription (Surface Copilot+ PC)")
    print("Note: This is a prototype implementation.")
    print("Full NPU acceleration requires ONNX Whisper models and QNN optimization.")
    print("")
    
    # Import the existing transcription function
    try:
        from transcribe import transcribe_file_with_options
        
        # Use CPU with a note about NPU potential
        options = {
            "model": model_size,
            "device": "cpu",  # Would be "qnn" with proper ONNX models
            "preprocess": True,
            "vad": True,
            "punctuate": True,
            "keep_temp": False
        }
        
        print(f"Transcribing {audio_file} with NPU-optimized settings...")
        print("(Currently running on CPU - NPU support coming soon)")
        
        # Set output directory
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            
        # Call the existing transcription function
        result = transcribe_file_with_options(audio_file, output_dir, options)
        
        print("\n‚úÖ Transcription completed!")
        print("üí° Future versions will use your Surface NPU for 2-5x speedup")
        
        return result
        
    except ImportError as e:
        print(f"Error: Cannot import transcription module: {e}")
        print("Make sure you're running this from the speech2textrme directory.")
        return None

def main():
    parser = argparse.ArgumentParser(description="NPU-accelerated transcription for Surface Copilot+ PCs")
    parser.add_argument("input", help="Input audio/video file")
    parser.add_argument("--output", "-o", help="Output directory", default=None)
    parser.add_argument("--model", choices=["small", "medium", "large"], default="medium",
                        help="Model size (default: medium)")
    parser.add_argument("--install-deps", action="store_true", 
                        help="Install NPU dependencies and exit")
    
    args = parser.parse_args()
    
    if args.install_deps:
        success = install_npu_dependencies()
        if success:
            print("‚úÖ NPU dependencies are installed and ready!")
        else:
            print("‚ùå Please restart the script after installation completes.")
        return
    
    # Check if input file exists
    if not os.path.exists(args.input):
        print(f"Error: Input file '{args.input}' not found.")
        return 1
    
    # Check NPU dependencies
    if not install_npu_dependencies():
        print("Please run with --install-deps first, then restart.")
        return 1
    
    # Transcribe with NPU
    result = transcribe_with_npu(args.input, args.model, args.output)
    
    if result:
        return 0
    else:
        return 1

if __name__ == "__main__":
    sys.exit(main())
